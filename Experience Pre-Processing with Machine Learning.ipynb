{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Getting Started</b>\n",
    "\n",
    "- Using a library called MoviePy, we will extract the audio from the video recording. And in the next step, we will convert that audio file into text using Google’s speech recognition library. If you are ready, let’s get started by installing the libraries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='https://edusync.s3.ams03.cloud-object-storage.appdomain.cloud/Video-2-Text.jpg' height='200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries<br/>\n",
    "\n",
    "   - Speech Recognition\n",
    "   - MoviePy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SpeechRecognition moviepy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SpeechRecognition module supports multiple recognition APIs, and Google Speech API is one of them</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MoviePy is a library that can read and write all the most common audio and video formats, including GIF. </b>\n",
    "- If you are having issues when installing moviepy library, try by installing ffmpeg. Ffmpeg is a leading multimedia framework, able to decode, encode, transcode, mux, demux, stream, filter and play pretty much anything that humans and machines have created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 — Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "import moviepy.editor as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 — Video to Audio Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = mp.VideoFileClip(r”video_recording.mov”) \n",
    " \n",
    "clip.audio.write_audiofile(r”converted.wav”)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 — Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()\n",
    "audio = sr.AudioFile(\"converted.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best part, which is recognizing the speech in an audio file. The recognizer will try to understand the speech and convert it to a text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with audio as source:\n",
    "  audio_file = r.record(source)\n",
    "result = r.recognize_google(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the intermediate Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the result \n",
    "with open('recognized.txt',mode ='w') as file: \n",
    "   file.write(\"Recognized Speech:\") \n",
    "   file.write(\"\\n\") \n",
    "   file.write(result) \n",
    "   print(\"ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Experience  from speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import json\n",
    "from os.path import join, dirname\n",
    "from ibm_watson import TextToSpeechV1\n",
    "from ibm_watson.websocket import SynthesizeCallback\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "authenticator = IAMAuthenticator('your_api_key')\n",
    "service = TextToSpeechV1(authenticator=authenticator)\n",
    "service.set_service_url('https://api.us-south.text-to-speech.watson.cloud.ibm.com')\n",
    "\n",
    "voices = service.list_voices().get_result()\n",
    "print(json.dumps(voices, indent=2))\n",
    "\n",
    "with open(join(dirname(__file__), '../resources/output.wav'),\n",
    "          'wb') as audio_file:\n",
    "    response = service.synthesize(\n",
    "        'Hello world!', accept='audio/wav',\n",
    "        voice=\"en-US_AllisonVoice\").get_result()\n",
    "    audio_file.write(response.content)\n",
    "\n",
    "pronunciation = service.get_pronunciation('Watson', format='spr').get_result()\n",
    "print(json.dumps(pronunciation, indent=2))\n",
    "\n",
    "voice_models = service.list_custom_models().get_result()\n",
    "print(json.dumps(voice_models, indent=2))\n",
    "\n",
    "# voice_model = service.create_custom_model('test-customization').get_result()\n",
    "# print(json.dumps(custom_model, indent=2))\n",
    "\n",
    "# updated_custom_model = service.update_custom_model(\n",
    "#     'YOUR CUSTOMIZATION ID', name='new name').get_result()\n",
    "# print(updated_custom_model)\n",
    "\n",
    "\n",
    "file_path = join(dirname(__file__), \"../resources/dog.wav\")\n",
    "class MySynthesizeCallback(SynthesizeCallback):\n",
    "    def __init__(self):\n",
    "        SynthesizeCallback.__init__(self)\n",
    "        self.fd = open(file_path, 'ab')\n",
    "\n",
    "    def on_connected(self):\n",
    "        print('Connection was successful')\n",
    "\n",
    "    def on_error(self, error):\n",
    "        print('Error received: {}'.format(error))\n",
    "\n",
    "    def on_content_type(self, content_type):\n",
    "        print('Content type: {}'.format(content_type))\n",
    "\n",
    "    def on_timing_information(self, timing_information):\n",
    "        print(timing_information)\n",
    "\n",
    "    def on_audio_stream(self, audio_stream):\n",
    "        self.fd.write(audio_stream)\n",
    "\n",
    "    def on_close(self):\n",
    "        self.fd.close()\n",
    "        print('Done synthesizing. Closing the connection')\n",
    "\n",
    "my_callback = MySynthesizeCallback()\n",
    "service.synthesize_using_websocket('I like to pet dogs',\n",
    "                                   my_callback,\n",
    "                                   accept='audio/wav',\n",
    "                                   voice='en-US_AllisonVoice'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiences sharing on EduSync Watson Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import AssistantV2\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "authenticator = IAMAuthenticator('your apikey')\n",
    "assistant = AssistantV2(\n",
    "    version='2018-09-20',\n",
    "    authenticator=authenticator)\n",
    "assistant.set_service_url('https://api.us-south.assistant.watson.cloud.ibm.com')\n",
    "\n",
    "#########################\n",
    "# Sessions\n",
    "#########################\n",
    "\n",
    "session = assistant.create_session(\"<YOUR ASSISTANT ID>\").get_result()\n",
    "print(json.dumps(session, indent=2))\n",
    "\n",
    "assistant.delete_session(\"<YOUR ASSISTANT ID>\", \"<YOUR SESSION ID>\").get_result()\n",
    "\n",
    "#########################\n",
    "# Message\n",
    "#########################\n",
    "\n",
    "message = assistant.message(\n",
    "    \"<YOUR ASSISTANT ID>\",\n",
    "    \"<YOUR SESSION ID>\",\n",
    "    input={'text': 'What\\'s the weather like?'},\n",
    "    context={\n",
    "        'metadata': {\n",
    "            'deployment': 'myDeployment'\n",
    "        }\n",
    "    }).get_result()\n",
    "print(json.dumps(message, indent=2))\n",
    "\n",
    "# logs = assistant.list_logs(\n",
    "#     \"<YOUR ASSISTANT ID>\"\n",
    "# )\n",
    "# print(json.dumps(logs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiences Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, EntitiesOptions, KeywordsOptions\n",
    "# from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "# Authentication via IAM\n",
    "# authenticator = IAMAuthenticator('your_api_key')\n",
    "# service = NaturalLanguageUnderstandingV1(\n",
    "#     version='2018-03-16',\n",
    "#     authenticator=authenticator)\n",
    "# service.set_service_url('https://gateway.watsonplatform.net/natural-language-understanding/api')\n",
    "\n",
    "# Authentication via external config like VCAP_SERVICES\n",
    "service = NaturalLanguageUnderstandingV1(\n",
    "    version='2018-03-16')\n",
    "service.set_service_url('https://api.us-south.natural-language-understanding.watson.cloud.ibm.com')\n",
    "\n",
    "response = service.analyze(\n",
    "    text='Bruce Banner is the Hulk and Bruce Wayne is BATMAN! '\n",
    "    'Superman fears not Banner, but Wayne.',\n",
    "    features=Features(entities=EntitiesOptions(),\n",
    "                      keywords=KeywordsOptions())).get_result()\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Visual Recognition on Experience sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from ibm_watson import VisualRecognitionV4\n",
    "from ibm_watson.visual_recognition_v4 import FileWithMetadata, TrainingDataObject, Location, AnalyzeEnums\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "authenticator = IAMAuthenticator(\n",
    "    'YOUR APIKEY')\n",
    "service = VisualRecognitionV4(\n",
    "    '2018-03-19',\n",
    "    authenticator=authenticator)\n",
    "service.set_service_url('https://api.us-south.visual-recognition.watson.cloud.ibm.com')\n",
    "\n",
    "# create a classifier\n",
    "my_collection = service.create_collection(\n",
    "    name='',\n",
    "    description='testing for python'\n",
    ").get_result()\n",
    "collection_id = my_collection.get('collection_id')\n",
    "\n",
    "# add images\n",
    "with open(os.path.join(os.path.dirname(__file__), '../resources/South_Africa_Luca_Galuzzi_2004.jpeg'), 'rb') as giraffe_info:\n",
    "    add_images_result = service.add_images(\n",
    "        collection_id,\n",
    "        images_file=[FileWithMetadata(giraffe_info)],\n",
    "    ).get_result()\n",
    "print(json.dumps(add_images_result, indent=2))\n",
    "image_id = add_images_result.get('images')[0].get('image_id')\n",
    "\n",
    "# add image training data\n",
    "training_data = service.add_image_training_data(\n",
    "    collection_id,\n",
    "    image_id,\n",
    "    objects=[\n",
    "        TrainingDataObject(object='giraffe training data',\n",
    "                           location=Location(64, 270, 755, 784))\n",
    "    ]).get_result()\n",
    "print(json.dumps(training_data, indent=2))\n",
    "\n",
    "# update object metadata\n",
    "updated_object_metadata = service.update_object_metadata(\n",
    "    collection_id=collection_id,\n",
    "    object='giraffe training data',\n",
    "    new_object='updated giraffe training data').get_result()\n",
    "print(json.dumps(updated_object_metadata, indent=2))\n",
    "\n",
    "# train collection\n",
    "train_result = service.train(collection_id).get_result()\n",
    "print(json.dumps(train_result, indent=2))\n",
    "\n",
    "# training usage\n",
    "training_usage = service.get_training_usage()\n",
    "print(json.dumps(training_usage, indent=2))\n",
    "\n",
    "# analyze\n",
    "dog_path = os.path.join(os.path.dirname(__file__), '../resources/dog.jpg')\n",
    "giraffe_path = os.path.join(os.path.dirname(__file__), '../resources/my-giraffe.jpeg')\n",
    "with open(dog_path, 'rb') as dog_file, open(giraffe_path, 'rb') as giraffe_files:\n",
    "    analyze_images = service.analyze(\n",
    "        collection_ids=[collection_id],\n",
    "        features=[AnalyzeEnums.Features.OBJECTS.value],\n",
    "        images_file=[\n",
    "            FileWithMetadata(dog_file),\n",
    "            FileWithMetadata(giraffe_files)\n",
    "        ],\n",
    "        image_url=['https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/American_Eskimo_Dog.jpg/1280px-American_Eskimo_Dog.jpg']).get_result()\n",
    "    print(json.dumps(analyze_images, indent=2))\n",
    "\n",
    "# delete collection\n",
    "service.delete_collection(collection_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
